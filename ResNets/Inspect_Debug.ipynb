{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  ['pablorr']\n",
      "Conda env:  ['/n/home01/pablorr/.conda/envs/torch37/bin/python']\n",
      "Python version:  ['Python 3.7.0']\n",
      "Cuda module:  ['cuda:']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nvcc: NVIDIA (R) Cuda compiler driver',\n",
       " 'Copyright (c) 2005-2017 NVIDIA Corporation',\n",
       " 'Built on Fri_Sep__1_21:08:03_CDT_2017',\n",
       " 'Cuda compilation tools, release 9.0, V9.0.176']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me = !whoami\n",
    "env = !which python\n",
    "pyv = !python -V\n",
    "cuda = !whereis cuda  \n",
    "cuda_version = !nvcc --version\n",
    "print('User: ', me)\n",
    "print('Conda env: ', env)\n",
    "print('Python version: ', pyv)\n",
    "print('Cuda module: ', cuda)\n",
    "cuda_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:             94           3          52           0          38          89\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from beautifultable import BeautifulTable as BT\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "bl = print('\\n') \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import load_dataset, count_parameters, figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION\n",
      "-------------\n",
      "+--------------------+------+\n",
      "| Python Interpreter |  0   |\n",
      "+--------------------+------+\n",
      "|   Python Version   |  0   |\n",
      "+--------------------+------+\n",
      "| Memory check (MBs) |  0   |\n",
      "+--------------------+------+\n",
      "|        Cuda        | True |\n",
      "+--------------------+------+\n",
      "|       Device       | cuda |\n",
      "+--------------------+------+\n",
      "|       Cores        |  48  |\n",
      "+--------------------+------+\n",
      "|        GPUs        |  2   |\n",
      "+--------------------+------+\n",
      "|   CUDNN Enabled    | True |\n",
      "+--------------------+------+\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "CONFIGURATION \n",
    "-------------\n",
    "\n",
    "Catch from the parser all the parameters to define the training\n",
    "'''\n",
    "print('CONFIGURATION')\n",
    "print('-------------')\n",
    "\n",
    "dataset = 'fruits-small'\n",
    "\n",
    "# Backup code to debug from python shell - no parser\n",
    "save = False                # Activate results saving \n",
    "draws = False               # Activate showing the figures\n",
    "testing = False             # Activate test to run few iterations per epoch       \n",
    "comments = True             # Activate printing comments\n",
    "createlog = False           # Activate option to save the logs in .txt\n",
    "save_frequency = 1          # After how many epochs save stats\n",
    "ensemble_type = 'Big'       # Single model big \n",
    "#ensemble_type = 'Huge'     # Single model huge\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "n_iters = 64000\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "n_epochs = int(n_iters / batch_size)\n",
    "\n",
    "# GPU if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "n_workers = multiprocessing.cpu_count()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpus = True if torch.cuda.device_count() > 1 else False\n",
    "mem = False if device == 'cpu' else True\n",
    "\n",
    "table = BT()\n",
    "table.append_row(['Python Interpreter', os.system(\"which python\")])\n",
    "table.append_row(['Python Version', os.system('python -V')])\n",
    "table.append_row(['Memory check (MBs)', os.system('free -m -h')])\n",
    "table.append_row(['Cuda', str(cuda)])\n",
    "table.append_row(['Device', str(device)])\n",
    "table.append_row(['Cores', str(n_workers)])\n",
    "table.append_row(['GPUs', str(torch.cuda.device_count())])\n",
    "table.append_row(['CUDNN Enabled', str(torch.backends.cudnn.enabled)])\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFINITION OF PATHS\n",
      "-------------------\n",
      "Root path:  /n/home01/pablorr/Single_vs_Ensemble_of_NNs\n",
      "Script path:  /n/home01/pablorr/Single_vs_Ensemble_of_NNs/ResNets\n",
      "Result path:  /n/home01/pablorr/Single_vs_Ensemble_of_NNs/results\n",
      "DataFolder path:  /n/home01/pablorr/datasets\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEFININTION OF PATHS \n",
    "--------------------\n",
    "Define all the paths to load / save files\n",
    "Ensure all those paths are correctly defined before moving on\n",
    "'''\n",
    "\n",
    "print('DEFINITION OF PATHS')\n",
    "print('-------------------')\n",
    "scripts = os.getcwd()\n",
    "root = os.path.abspath(os.path.join(scripts, '../'))\n",
    "results = os.path.abspath(os.path.join(root, 'results'))\n",
    "data_path = os.path.abspath(os.path.join(root, '../datasets'))\n",
    "\n",
    "path_to_logs = os.path.join(results, 'logs', 'resnets')\n",
    "path_to_models = os.path.join(results, 'models', 'resnets')\n",
    "path_to_figures = os.path.join(results, 'figures', 'resnets')\n",
    "path_to_dataframes = os.path.join(results, 'dataframes', 'resnets')\n",
    "\n",
    "train_log = os.path.join(path_to_logs, 'train')\n",
    "test_log = os.path.join(path_to_logs, 'test')\n",
    "\n",
    "print('Root path: ', root)\n",
    "print('Script path: ', scripts)\n",
    "print('Result path: ', results)\n",
    "print('DataFolder path: ', data_path)\n",
    "\n",
    "assert os.path.exists(root), 'Root folder not found'\n",
    "assert os.path.exists(scripts), 'Scripts folder not found'\n",
    "assert os.path.exists(results), 'Results folder not found'\n",
    "assert os.path.exists(data_path), 'Data folder not found'\n",
    "assert os.path.exists(path_to_logs), 'Logs folder not found'\n",
    "assert os.path.exists(path_to_models), 'Models folder not found'\n",
    "assert os.path.exists(path_to_figures), 'Figure folder not found'\n",
    "assert os.path.exists(path_to_dataframes), 'Dataframes folder not found'\n",
    "\n",
    "paths = {\n",
    "        'root': root, \n",
    "        'script': scripts,\n",
    "        'data': data_path,\n",
    "        'resulsts': results,\n",
    "        'logs': {'train': train_log, 'test': test_log}, \n",
    "        'models': path_to_models,\n",
    "        'figures': path_to_figures,\n",
    "        'dataframes': path_to_dataframes\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING DATA\n",
      "--------------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not found folder for this particular dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4b155cfd4787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_loader = DataLoader(dataset = train_set.dataset, \n",
      "\u001b[0;32m~/Single_vs_Ensemble_of_NNs/utils.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(data_path, dataset, comments)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Exists particular data folder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CIFAR10'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not found folder for this particular dataset"
     ]
    }
   ],
   "source": [
    "# 1 - Import the Dataset\n",
    "# ----------------------\n",
    "print('IMPORTING DATA')\n",
    "print('--------------')\n",
    "\n",
    "train_set, valid_set, test_set = load_dataset(data_path, dataset, comments=comments)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set.dataset, \n",
    "                          sampler=SubsetRandomSampler(train_set.indices),\n",
    "                          batch_size = batch_size, num_workers=n_workers,\n",
    "                          pin_memory = mem)\n",
    "\n",
    "valid_loader = DataLoader(dataset = valid_set.dataset, \n",
    "                          sampler=SubsetRandomSampler(valid_set.indices),\n",
    "                          batch_size = batch_size, num_workers=n_workers,\n",
    "                          pin_memory = mem)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set, batch_size = 1,\n",
    "                         shuffle = False, num_workers=n_workers, pin_memory = mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
