{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset training to reach state-of-art results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from beautifultable import BeautifulTable as BT\n",
    "\n",
    "# Switching multiprocessing to avoid \"Too many files opened\"\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('ResNets')\n",
    "from utils import load_dataset, count_parameters\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', 'ImportWarning')\n",
    "warnings.filterwarnings('ignore', 'DeprecationWarning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CONFIGURATION\n",
      "-------------\n",
      "+-----------------+-------+\n",
      "| Python Version  | 3.6.5 |\n",
      "+-----------------+-------+\n",
      "| PyTorch Version | 1.0.0 |\n",
      "+-----------------+-------+\n",
      "|      Cuda       | True  |\n",
      "+-----------------+-------+\n",
      "|     Device      | cuda  |\n",
      "+-----------------+-------+\n",
      "|      Cores      |   4   |\n",
      "+-----------------+-------+\n",
      "|      GPUs       |   1   |\n",
      "+-----------------+-------+\n",
      "|  CUDNN Enabled  | True  |\n",
      "+-----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "CONFIGURATION \n",
    "-------------\n",
    "\n",
    "Catch from the parser all the parameters to define the training\n",
    "'''\n",
    "print('\\n\\nCONFIGURATION')\n",
    "print('-------------')\n",
    "\n",
    "dataset = 'CIFAR10'\n",
    "testing = False             # Activate test to run few iterations per epoch       \n",
    "comments = True             # Activate printing comments\n",
    "ensemble_type = 'Big'       # Single model big \n",
    "#ensemble_type = 'Huge'     # Single model huge\n",
    "batch_size = 128\n",
    "n_epochs = 300\n",
    "n_iters = 64000\n",
    "learning_rate = 0.001\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# GPU if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "n_workers = multiprocessing.cpu_count()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpus = True if torch.cuda.device_count() > 1 else False\n",
    "mem = False if device == 'cpu' else True\n",
    "\n",
    "table = BT()\n",
    "table.append_row(['Python Version', sys.version[:5]])\n",
    "table.append_row(['PyTorch Version', torch.__version__])\n",
    "table.append_row(['Cuda', str(cuda)])\n",
    "table.append_row(['Device', str(device)])\n",
    "table.append_row(['Cores', str(n_workers)])\n",
    "table.append_row(['GPUs', str(torch.cuda.device_count())])\n",
    "table.append_row(['CUDNN Enabled', str(torch.backends.cudnn.enabled)])\n",
    "print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  8 07:53:44 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   37C    P8    25W / 149W |     11MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFINITION OF PATHS\n",
      "-------------------\n",
      "Paths Validated\n",
      "---------------\n",
      "Root path:  /home/ec2-user/Single_vs_Ensemble_of_NNs\n",
      "Script path:  /home/ec2-user/Single_vs_Ensemble_of_NNs/ResNets\n",
      "Results path:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results\n",
      "DataFolder path:  /home/ec2-user/datasets\n",
      "Models to save path:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results/models/resnets\n",
      "Models to load path:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results/models/resnets/definitives\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEFININTION OF PATHS \n",
    "--------------------\n",
    "Define all the paths to load / save files\n",
    "Ensure all those paths are correctly defined before moving on\n",
    "'''\n",
    "\n",
    "print('DEFINITION OF PATHS')\n",
    "print('-------------------')\n",
    "scripts = os.getcwd()\n",
    "root = os.path.abspath(os.path.join(scripts, '../'))\n",
    "results = os.path.abspath(os.path.join(root, 'results'))\n",
    "data_path = os.path.abspath(os.path.join(root, '../datasets'))\n",
    "\n",
    "path_to_logs = os.path.join(results, 'logs', 'resnets')\n",
    "path_to_models = os.path.join(results, 'models', 'resnets')\n",
    "path_to_figures = os.path.join(results, 'figures', 'resnets')\n",
    "path_to_definitives = os.path.join(path_to_models, 'definitives')\n",
    "\n",
    "train_log = os.path.join(path_to_logs, 'train')\n",
    "test_log = os.path.join(path_to_logs, 'test')\n",
    "\n",
    "assert os.path.exists(root), 'Root folder not found'\n",
    "assert os.path.exists(scripts), 'Scripts folder not found'\n",
    "assert os.path.exists(results), 'Results folder not found'\n",
    "assert os.path.exists(data_path), 'Data folder not found'\n",
    "assert os.path.exists(path_to_logs), 'Logs folder not found'\n",
    "assert os.path.exists(path_to_models), 'Models folder not found'\n",
    "assert os.path.exists(path_to_figures), 'Figure folder not found'\n",
    "assert os.path.exists(path_to_definitives), 'Def. models folder not found'\n",
    "\n",
    "print('Paths Validated')\n",
    "print('---------------')\n",
    "print('Root path: ', root)\n",
    "print('Script path: ', scripts)\n",
    "print('Results path: ', results)\n",
    "print('DataFolder path: ', data_path)\n",
    "print('Models to save path: ', path_to_models)\n",
    "print('Models to load path: ', path_to_definitives)\n",
    "\n",
    "paths = {\n",
    "    'root': root, \n",
    "    'script': scripts,\n",
    "    'data': data_path,\n",
    "    'resulsts': results,\n",
    "    'logs': {'train': train_log, 'test': test_log}, \n",
    "    'models': path_to_models,\n",
    "    'definitives': path_to_definitives,\n",
    "    'figures': path_to_figures\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING DATA\n",
      "--------------\n",
      "Files already downloaded and verified\n",
      "Loading dataset:  CIFAR10\n",
      "+--------------+-------+\n",
      "| Train Images | 45000 |\n",
      "+--------------+-------+\n",
      "| Valid Images | 5000  |\n",
      "+--------------+-------+\n",
      "| Test Images  | 10000 |\n",
      "+--------------+-------+\n",
      "|   Classes    |  10   |\n",
      "+--------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# 1 - Import the Dataset\n",
    "# ----------------------\n",
    "\n",
    "print('IMPORTING DATA')\n",
    "print('--------------')\n",
    "\n",
    "dataset = 'CIFAR10'\n",
    "comments=True\n",
    "train_set, valid_set, test_set = load_dataset(data_path, dataset, comments=comments)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set.dataset, \n",
    "                          sampler=SubsetRandomSampler(train_set.indices),\n",
    "                          batch_size = batch_size, num_workers=n_workers,\n",
    "                          pin_memory = mem)\n",
    "\n",
    "valid_loader = DataLoader(dataset = valid_set.dataset, \n",
    "                          sampler=SubsetRandomSampler(valid_set.indices),\n",
    "                          batch_size = batch_size, num_workers=n_workers,\n",
    "                          pin_memory = mem)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set, batch_size = 1,\n",
    "                         shuffle = False, num_workers=n_workers, pin_memory = mem)\n",
    "\n",
    "\n",
    "batches = len(train_loader)\n",
    "samples = len(train_loader.sampler.indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "IMPORTING MODELS\n",
      "----------------\n",
      "+------------+-------------+-----------------+\n",
      "|   Model    | M. Paramars | % over ResNet20 |\n",
      "+------------+-------------+-----------------+\n",
      "| ResNet 20  |    0.272    |       1.0       |\n",
      "+------------+-------------+-----------------+\n",
      "| ResNet 32  |    0.467    |      1.714      |\n",
      "+------------+-------------+-----------------+\n",
      "| ResNet 44  |    0.661    |      2.427      |\n",
      "+------------+-------------+-----------------+\n",
      "| ResNet 56  |    0.856    |      3.141      |\n",
      "+------------+-------------+-----------------+\n",
      "| ResNet 110 |    1.731    |      6.352      |\n",
      "+------------+-------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# 2 - Import the ResNet\n",
    "# ---------------------\n",
    "\n",
    "print('\\n\\nIMPORTING MODELS')\n",
    "print('----------------')\n",
    "\n",
    "from resnets_Paper import ResNet20, ResNet32, ResNet44, ResNet56, ResNet110\n",
    "\n",
    "resnet20 = ResNet20()\n",
    "resnet32 = ResNet32()\n",
    "resnet44 = ResNet44()\n",
    "resnet56 = ResNet56()\n",
    "resnet110 = ResNet110()\n",
    "\n",
    "def parameters(model, typ=None):\n",
    "    def compare_to_simplest(model, typ):\n",
    "        simplest = count_parameters(resnet20)\n",
    "        if typ is None: return count_parameters(model) / simplest\n",
    "    return count_parameters(model)*1e-6, compare_to_simplest(model, typ)\n",
    "\n",
    "\n",
    "table = BT()\n",
    "table.append_row(['Model', 'M. Paramars', '% over ResNet20'])\n",
    "table.append_row(['ResNet 20', *parameters(resnet20)])\n",
    "table.append_row(['ResNet 32', *parameters(resnet32)])\n",
    "table.append_row(['ResNet 44', *parameters(resnet44)])\n",
    "table.append_row(['ResNet 56', *parameters(resnet56)])\n",
    "table.append_row(['ResNet 110', *parameters(resnet110)])\n",
    "if comments: print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constraint - Parameters constant\n",
    "\n",
    "small = count_parameters(ResNet20())  # 3:1 vs 6:1\n",
    "singleModel = ResNet56() if ensemble_type == 'Big' else ResNet110() \n",
    "ensemble_size = round(count_parameters(singleModel) / small)\n",
    "\n",
    "\n",
    "# Construct the single model\n",
    "\n",
    "singleModel = ResNet56() if ensemble_type == 'Big' else ResNet110() # 3:1 vs 6:1\n",
    "title = singleModel.name\n",
    "\n",
    "name = singleModel.name\n",
    "singleModel.to(device)\n",
    "if gpus: singleModel = nn.DataParallel(singleModel)\n",
    "optimizer = optim.SGD(singleModel.parameters(), learning_rate, momentum, weight_decay)\n",
    "\n",
    "\n",
    "# Construct the ensemble\n",
    "\n",
    "names = []\n",
    "ensemble = []\n",
    "optimizers = []\n",
    "for i in range(ensemble_size):\n",
    "    \n",
    "    model = ResNet20()\n",
    "    names.append(model.name + '_' + str(i+1))\n",
    "    params = optim.SGD(model.parameters(), learning_rate, momentum, weight_decay)\n",
    "    optimizers.append(params)\n",
    "    \n",
    "    model.to(device)\n",
    "    if gpus: model = nn.DataParallel(model)\n",
    "    ensemble.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained models... \n",
      "Files to load:\n",
      "... snet56/ResNet20_3-149.pkl\n",
      "... snet56/ResNet20_2-149.pkl\n",
      "... resnet56/ResNet56-161.pkl\n",
      "... snet56/ResNet20_1-149.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load Best Models\n",
    "# ----------------\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_epoch(pth):\n",
    "    pth = pth.split('/')[-1] # remove all the path\n",
    "    pth = pth[:-4] # remove .pkl\n",
    "    epoch = pth.split('-')[1] # get just the epoch\n",
    "    print('Epoch to restart training: ', epoch)\n",
    "    return epoch\n",
    "\n",
    "def load_weights(path, verbose=0):\n",
    "    global device\n",
    "    print('\\n\\nLoading Weights from: ', path)\n",
    "    print('-------')\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    if verbose == 1: print('\\nCurrent dict: ', state_dict.keys())\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k,v in state_dict.items():\n",
    "        name = k[7:] # remove module.\n",
    "        new_state_dict[name] = v\n",
    "    if verbose == 1: print('\\nNew dict: ', new_state_dict.keys())\n",
    "    return new_state_dict\n",
    "\n",
    "## LOAD TRAINED MODELS      -->         args.pretrained = -P = True\n",
    "print('Loading trained models... ')\n",
    "\n",
    "# Load saved models\n",
    "if ensemble_type == 'Big':\n",
    "    pth = os.path.join(path_to_definitives, 'resnet56')\n",
    "else:\n",
    "    pth = os.path.join(path_to_definitives, 'resnet110')\n",
    "    e_epoch = get_epoch(pth)\n",
    "\n",
    "assert os.path.exists(pth), 'Model to load not found'\n",
    "\n",
    "ps = glob.glob(os.path.join(pth, '*.pkl'))\n",
    "print('Files to load:')\n",
    "for p in ps:\n",
    "    print('...', p[-25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single:  resnet56/ResNet56-161.pkl\n"
     ]
    }
   ],
   "source": [
    "single_ps = [p for p in ps if 'ResNet56' in p]\n",
    "ensemble_ps = [p for p in ps if 'ResNet56' not in p]\n",
    "print('Single: ', single_ps[0][-25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ready Single Model :  ResNet(\n",
      "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Epoch to restart training:  161\n",
      "\n",
      "\n",
      "Loading Weights from:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results/models/resnets/definitives/resnet56/ResNet56-161.pkl\n",
      "-------\n",
      "[OK] Single model loaded on epoch  161\n"
     ]
    }
   ],
   "source": [
    "# Single Model\n",
    "print('Getting ready Single Model : ', singleModel)\n",
    "s_epoch = int(get_epoch(single_ps[0]))\n",
    "singleModel.load_state_dict(load_weights(single_ps[0], verbose=0))\n",
    "print('[OK] Single model loaded on epoch ', s_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch to restart training:  149\n",
      "\n",
      "\n",
      "Loading Weights from:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results/models/resnets/definitives/resnet56/ResNet20_3-149.pkl\n",
      "-------\n",
      "\n",
      "\n",
      "Loading Weights from:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results/models/resnets/definitives/resnet56/ResNet20_2-149.pkl\n",
      "-------\n",
      "\n",
      "\n",
      "Loading Weights from:  /home/ec2-user/Single_vs_Ensemble_of_NNs/results/models/resnets/definitives/resnet56/ResNet20_1-149.pkl\n",
      "-------\n",
      "[OK] Ensemble loaded on epoch  149\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Members\n",
    "e_epoch = int(get_epoch(ensemble_ps[0]))\n",
    "for i,p in enumerate(ensemble_ps):                \n",
    "    ensemble[i].load_state_dict(load_weights(p, verbose=0))  \n",
    "print('[OK] Ensemble loaded on epoch ', e_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset training of Single Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TRAINING\n",
      "--------\n",
      "Starting Single Model Training...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e15f7ece79f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m           valid_loader, e_epoch, n_epochs, n_iters, save, paths, testing]\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Results_Loaded_Single_Models.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mobject_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Single_vs_Ensemble_of_NNs/ResNets/train_reset.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, name, model, optimizer, criterion, device, trainloader, validloader, epoch, epochs, iters, save, paths, test, validate)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# for printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_init_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_init_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_queue_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_result_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_pids_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36mQueue\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m'''Returns a queue object'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mJoinableQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSEM_VALUE_MAX\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maxsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduplex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mPipe\u001b[0;34m(duplex)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mfd1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwritable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 24] Too many open files"
     ]
    }
   ],
   "source": [
    "# Reset Models from saved Epoch\n",
    "# -----------------------------\n",
    "\n",
    "print('\\n\\nTRAINING')\n",
    "print('--------')\n",
    "\n",
    "save = False\n",
    "criterion = nn.CrossEntropyLoss().cuda() if cuda else nn.CrossEntropyLoss()\n",
    "\n",
    "# Big Single Model\n",
    "     \n",
    "cudnn.benchmark = False    \n",
    "cudnn.benchmark = True\n",
    "\n",
    "from train_reset import train as tr\n",
    "print('Starting Single Model Training...' )\n",
    "params = [dataset, name, singleModel, optimizer, criterion, device, train_loader,\n",
    "          valid_loader, s_epoch, n_epochs, n_iters, save, paths, testing]\n",
    "\n",
    "results = tr(*params)\n",
    "with open('Results_Loaded_Single_Models.pkl', 'wb') as object_result:\n",
    "    pickle.dump(results, object_result, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset training of Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model\n",
    "    \n",
    "cudnn.benchmark = False    \n",
    "cudnn.benchmark = True\n",
    "from train_reset_ensemble import train as tre\n",
    "print('Starting Ensemble Training...')\n",
    "\n",
    "params = [dataset, names, ensemble, optimizers, criterion, device, train_loader,\n",
    "          valid_loader, e_epoch, n_epochs, n_iters, save, paths, testing]\n",
    "\n",
    "ens_results = tre(*params)\n",
    "with open('Results_Loaded_Ensemble_Models.pkl', 'wb') as object_result:\n",
    "    pickle.dump(ens_results, object_result, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "ens_results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
